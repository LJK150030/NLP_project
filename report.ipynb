{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Valence Assignment for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_known_file = 1\n",
    "last_known_file = 67790\n",
    "#last_known_file = 1000\n",
    "max_file_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for web_idx in range(first_known_file, last_known_file):\n",
    "    url = f\"https://gutenberg.org/files/{web_idx}/\"\n",
    "    try:\n",
    "        file = urllib.request.urlopen(url)\n",
    "    except urllib.error.HTTPError as e: # If the page is forbiden, then move on\n",
    "        continue\n",
    "    except urllib.error.URLError as e: # if we loose connection, try again\n",
    "        time.sleep(4)\n",
    "        web_idx -= 4\n",
    "\n",
    "    # Looking for every .txt.utf-8 file given the above url\n",
    "    online_file = list()\n",
    "    for line in file:\n",
    "        decoded_line = None\n",
    "        try:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            continue\n",
    "\n",
    "        online_file.append(decoded_line)\n",
    "\n",
    "\n",
    "\n",
    "    # For each .txt.utf-8 file we need to create a new url to get to the file\n",
    "    res = [string for string in online_file if '.txt\">' in string]\n",
    "    txt_urls = list()\n",
    "    txt_file_name = None\n",
    "    for line in res:\n",
    "        result = re.search('<a href=\"(.*).txt\">', line)\n",
    "        if result != None:\n",
    "            num_groups = len(result.groups())\n",
    "\n",
    "            if num_groups > 0:\n",
    "                txt_file_name = f\"{result.group(1)}.txt\"\n",
    "                txt_urls.append(f\"{url}{txt_file_name}\")\n",
    "\n",
    "    # for each .txt.utf-8 url file, read in the file and get all of the text\n",
    "    txt_file_content = list()\n",
    "    line_idx = 0\n",
    "    for url_idx in range(len(txt_urls)):\n",
    "        url = txt_urls[url_idx]\n",
    "\n",
    "        if url == None or url == '':\n",
    "            continue\n",
    "\n",
    "        file = urllib.request.urlopen(url)\n",
    "        line_by_line = list()\n",
    "        for line in file:\n",
    "            try:\n",
    "                decoded_line = line.decode(\"utf-8\")\n",
    "            except UnicodeDecodeError as e:\n",
    "                continue\n",
    "            line_by_line.append(decoded_line)  \n",
    "        txt_file_content.append(line_by_line)\n",
    "\n",
    "        size_of_file = sys.getsizeof(line_by_line)\n",
    "        if size_of_file > max_file_size:\n",
    "            max_file_size = size_of_file\n",
    "        print(f\"{url}: {size_of_file} / {max_file_size}\")\n",
    "    \n",
    "    # Write file to local disk\n",
    "    #txt_file_content_np = np.asarray(txt_file_content, str)\n",
    "    #txt_file_content_np = txt_file_content_np.flatten()\n",
    "    if len(txt_file_content) >= 1 and len(txt_file_content[0]) >= 1:\n",
    "        with open(f'raw/{txt_file_name}','w') as f:\n",
    "            if f.tell() < 1024*1024*1024:\n",
    "                #pickle.dump(txt_file_content_np, f)\n",
    "                f.write(json.dumps(txt_file_content))\n",
    "                #for line in txt_file_content:\n",
    "                    #f.write('%s\\n' % line)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('raw/10.pkl','rb') as f:\n",
    "#    x = pickle.load(f)\n",
    "#    print(x[0][125])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example document\n",
    "\n",
    "\n",
    "## ---------------\n",
    "\n",
    "\n",
    "The Project Gutenberg EBook of Madame de Ferneuse, by Daniel Lesueur\n",
    "\n",
    "This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you'll have to check the laws of the country where you are located before using this ebook.\n",
    "\n",
    "Title: Madame de Ferneuse\n",
    "\n",
    "Author: Daniel Lesueur\n",
    "\n",
    "Release Date: March 21, 2016 [EBook #51515]\n",
    "\n",
    "Language: French\n",
    "\n",
    "Character set encoding: UTF-8\n",
    "\n",
    "*** START OF THIS PROJECT GUTENBERG EBOOK MADAME DE FERNEUSE ***\n",
    "\n",
    "*** END OF THIS PROJECT GUTENBERG EBOOK MADAME DE FERNEUSE ***\n",
    "\n",
    "***** This file should be named 51515-0.txt or 51515-0.zip ***** This and all associated files of various formats will be found in: http://www.gutenberg.org/5/1/5/1/51515/\n",
    "\n",
    "\n",
    "## ------------------\n",
    "\n",
    "The Project Gutenberg EBook of The Meaning of Infancy, by John Fiske\n",
    "\n",
    "This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org\n",
    "\n",
    "Title: The Meaning of Infancy\n",
    "\n",
    "Author: John Fiske\n",
    "\n",
    "Release Date: May 15, 2004 [EBook #12359]\n",
    "\n",
    "Language: English\n",
    "\n",
    "Character set encoding: ASCII\n",
    "\n",
    "*** START OF THIS PROJECT GUTENBERG EBOOK THE MEANING OF INFANCY ***\n",
    "\n",
    "*** END OF THIS PROJECT GUTENBERG EBOOK THE MEANING OF INFANCY ***\n",
    "\n",
    "## -------------------\n",
    "\n",
    "The Project Gutenberg eBook of Traumerei, by Charles Beaumont\n",
    "\n",
    "This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.\n",
    "\n",
    "Title: Traumerei\n",
    "\n",
    "Author: Charles Beaumont\n",
    "\n",
    "Release Date: February 7, 2022 [eBook #67341]\n",
    "\n",
    "Language: English\n",
    "\n",
    "Produced by: Greg Weeks, Mary Meehan and the Online Distributed Proofreading Team at http://www.pgdp.net\n",
    "\n",
    "*** START OF THE PROJECT GUTENBERG EBOOK TRAUMEREI ***\n",
    "\n",
    "*** END OF THE PROJECT GUTENBERG EBOOK TRAUMEREI ***\n",
    "\n",
    "## -------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Design\n",
    "Right now, we are downloading all of the files, saving them as raw array format as a json file.\n",
    "\n",
    "Alternative approach as relational database\n",
    "We can generate the hypterlink by walking a list from 0 to last_known_file. We would need to check what files they have. Then, save only the heading information in each document, and remember where the starting index of the literature as well as the ending index.  \n",
    "\n",
    "Format tipes:\n",
    "<ul>\n",
    "    <li>.txt</li>\n",
    "    <li>-0.txt</li>\n",
    "    <li>-8.txt</li>\n",
    "    <li>-h/</li>\n",
    "    <li>-0.zip</li>\n",
    "    <li>-8.zip</li>\n",
    "    <li>-h.zip</li>\n",
    "</ul>\n",
    "\n",
    "from there, we can read the txt file, assuming it can be decoded using utf-8. The reoccuring headings can allow us to save data entries into a relational database.\n",
    "Document: (some of these fields may not be nan)\n",
    "<ul>\n",
    "    <li>Title:</li>\n",
    "    <li>Author:</li>\n",
    "    <li>URL:</li>\n",
    "    <li>Release Date:</li>\n",
    "    <li>Edition:</li>\n",
    "    <li>Language:</li>\n",
    "    <li>Produced by:</li>\n",
    "    <li>Start_Of_doc_indx:</li>\n",
    "    <li>End_of_doc_indx:</li>\n",
    "    <li>(or, if we have the memory, the entire document, but for now assuming we are reading from a URL or File Stream)</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python and SQLite3\n",
    "\n",
    "Medium post by Christopher Tao: https://towardsdatascience.com/do-you-know-python-has-a-built-in-database-d553989c87bd\n",
    "\n",
    "Tutorialspoint python SQLlite: https://towardsdatascience.com/do-you-know-python-has-a-built-in-database-d553989c87bd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite database  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example code to work with sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sl.connect('my-test.db')\n",
    "\n",
    "with con:\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE USER (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT,\n",
    "            age INTEGER\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "sql = 'INSERT INTO USER (id, name, age) values(?, ?, ?)'\n",
    "data = [\n",
    "    (1, 'Alice', 21),\n",
    "    (2, 'Bob', 22),\n",
    "    (3, 'Chris', 23)\n",
    "]\n",
    "\n",
    "with con:\n",
    "    con.executemany(sql, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with con:\n",
    "    data = con.execute(\"SELECT * FROM USER WHERE age <= 22\")\n",
    "    for row in data:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skill = pd.DataFrame({\n",
    "    'user_id': [1,1,2,2,3,3,3],\n",
    "    'skill': ['Network Security', 'Algorithm Development', 'Network Security', 'Java', 'Python', 'Data Science', 'Machine Learning']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skill.to_sql('SKILL', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('''\n",
    "    SELECT s.user_id, u.name, u.age, s.skill \n",
    "    FROM USER u LEFT JOIN SKILL s ON u.id = s.user_id\n",
    "''', con)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('USER_SKILL', con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Refactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con = sl.connect('gutenberg.db')\n",
    "\n",
    "with db_con:\n",
    "    db_con.execute(\"\"\"\n",
    "        CREATE TABLE LITERATURE (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT,\n",
    "            author TEXT,\n",
    "            root_url TEXT,\n",
    "            release_date TEXT,\n",
    "            edition INTEGER,\n",
    "            language TEXT,\n",
    "            produced_by TEXT,\n",
    "            translator TEXT,\n",
    "            url_txt_0 TEXT,\n",
    "            url_txt_8 TEXT,\n",
    "            url_zip_0 TEXT,\n",
    "            url_zip_8 TEXT,\n",
    "            url_zip_h TEXT,\n",
    "            url_h TEXT,\n",
    "            url_txt TEXT\n",
    "        );\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sql = 'INSERT INTO USER (id, name, age) values(?, ?, ?)'\n",
    "#data = [\n",
    "#    (1, 'Alice', 21),\n",
    "#    (2, 'Bob', 22),\n",
    "#    (3, 'Chris', 23)\n",
    "#]\n",
    "#\n",
    "#with con:\n",
    "#    con.executemany(sql, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db_con = sl.connect('gutenberg.db')\n",
    "#\n",
    "#with con:\n",
    "#    con.execute(\"\"\"DROP table LITERATURE;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_0_FILE = '-0.txt'\n",
    "TXT_8_FILE = '-8.txt'\n",
    "ZIP_0_FILE = '-0.zip'\n",
    "ZIP_8_FILE = '-8.zip'\n",
    "ZIP_H_FILE = '-h.zip'\n",
    "H_FILE = '-h/'\n",
    "TXT_FILE = '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for web_idx in range(1, last_known_file):\n",
    "    # variables to populate in database\n",
    "    id = web_idx\n",
    "    title = \"None\"\n",
    "    author = \"None\"\n",
    "    root_url = f\"https://gutenberg.org/files/{web_idx}/\"\n",
    "    release_date = \"None\"\n",
    "    edition = 0\n",
    "    language = \"None\"\n",
    "    produced_by = \"None\"\n",
    "    translator = \"None\"\n",
    "    url_txt_0 = \"None\"\n",
    "    url_txt_8 = \"None\"\n",
    "    url_zip_0 = \"None\"\n",
    "    url_zip_8 = \"None\"\n",
    "    url_zip_h = \"None\"\n",
    "    url_h = \"None\"\n",
    "    url_txt = \"None\"\n",
    "\n",
    "    # Connect to project gutenberg\n",
    "    try:\n",
    "        file = urllib.request.urlopen(root_url)\n",
    "    except urllib.error.HTTPError as e: # If the page is forbiden, then move on\n",
    "        continue\n",
    "    except urllib.error.URLError as e: # if we loose connection, try again\n",
    "        time.sleep(4)\n",
    "        web_idx -= 4\n",
    "\n",
    "    # Strip web string from php syntax\n",
    "    online_file = list()\n",
    "    for line in file:\n",
    "        decoded_line = None\n",
    "        try:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            continue\n",
    "        readable_line = ''.join(BeautifulSoup(decoded_line).findAll(text=True))\n",
    "        online_file.append(readable_line)\n",
    "    fullstring = ''.join(online_file)\n",
    "\n",
    "    # Parese web file for txt files  \n",
    "    if f'{web_idx}{TXT_0_FILE}' in fullstring:\n",
    "        url_txt_0 = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{TXT_0_FILE}\"\n",
    "    if f'{web_idx}{TXT_8_FILE}' in fullstring:\n",
    "        url_txt_8 = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{TXT_8_FILE}\"\n",
    "    if f'{web_idx}{ZIP_0_FILE}' in fullstring:\n",
    "        url_zip_0 = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{ZIP_0_FILE}\"\n",
    "    if f'{web_idx}{ZIP_8_FILE}' in fullstring:\n",
    "        url_zip_8 = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{ZIP_8_FILE}\"\n",
    "    if f'{web_idx}{ZIP_H_FILE}' in fullstring:\n",
    "        url_zip_h = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{ZIP_H_FILE}\"\n",
    "    if f'{web_idx}{H_FILE}' in fullstring:\n",
    "        url_h = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{H_FILE}\"\n",
    "    if f'{web_idx}{TXT_FILE}' in fullstring:\n",
    "        url_txt = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{TXT_FILE}\"\n",
    "\n",
    "    # use one of the txt files to gather remanining variables\n",
    "    file_url = None\n",
    "    if url_txt_0:\n",
    "        file_url = url_txt_0\n",
    "    elif url_txt_8:\n",
    "        file_url = url_txt_8\n",
    "    elif url_txt:\n",
    "        file_url = url_txt\n",
    "\n",
    "    if file_url == \"None\":\n",
    "        continue\n",
    "\n",
    "    # Connect to txt file\n",
    "    file = None\n",
    "    try:\n",
    "        file = urllib.request.urlopen(file_url)\n",
    "    except urllib.error.HTTPError as e: # If the page is forbiden, then move on\n",
    "        continue\n",
    "    except urllib.error.URLError as e: # if we loose connection, try again\n",
    "        time.sleep(4)\n",
    "        web_idx -= 4\n",
    "\n",
    "    # Strip txt file from php syntax\n",
    "    txt_file = list()\n",
    "    for line in file:\n",
    "        decoded_line = None\n",
    "        try:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            continue\n",
    "        #readable_line = ''.join(BeautifulSoup(decoded_line).findAll(text=True))\n",
    "        txt_file.append(decoded_line)\n",
    "    full_txt = ''.join(txt_file)\n",
    "\n",
    "    #print(full_txt)\n",
    "\n",
    "    # Parse text file for\n",
    "    title_results = re.search('Title: (.*)', full_txt)\n",
    "    author_results = re.search('Author: (.*)', full_txt)\n",
    "    release_data_results = re.search('Release Date: (.*) \\[', full_txt)\n",
    "    edition_results = re.search('Edition: (.*)', full_txt)\n",
    "    language_results = re.search('Language: (.*)', full_txt)\n",
    "    produced_results = re.search('Produced by: (.*)', full_txt)\n",
    "    translator_results = re.search('Translator: (.*)', full_txt)\n",
    "\n",
    "    if title_results:\n",
    "        title = str(title_results.group(1)).strip()\n",
    "    if author_results:\n",
    "        author = str(author_results.group(1)).strip()\n",
    "    if release_data_results:\n",
    "        release_date = str(release_data_results.group(1)).strip()\n",
    "    if edition_results:\n",
    "        edition_str = str(edition_results.group(1)).strip()\n",
    "        if edition_str.isnumeric():\n",
    "            edition = int(edition_str)\n",
    "    if language_results:\n",
    "        language = str(language_results.group(1)).strip()\n",
    "    if produced_results:\n",
    "        produced_by = str(produced_results.group(1)).strip()\n",
    "    if translator_results:\n",
    "        translator = str(translator_results.group(1)).strip()\n",
    "\n",
    "\n",
    "    #print(f\"id: {type(id)}\")\n",
    "    #print(f\"title: {type(title)}\")\n",
    "    #print(f\"author: {type(author)}\")\n",
    "    #print(f\"root_url: {type(root_url)}\")\n",
    "    #print(f\"release_date: {type(release_date)}\")\n",
    "    #print(f\"edition: {type(edition)}\")\n",
    "    #print(f\"language: {type(language)}\")\n",
    "    #print(f\"produced_by: {type(produced_by)}\")\n",
    "    #print(f\"url_txt_0: {type(url_txt_0)}\")\n",
    "    #print(f\"url_txt_8: {type(url_txt_8)}\")\n",
    "    #print(f\"url_zip_0: {type(url_zip_0)}\")\n",
    "    #print(f\"url_zip_8: {type(url_zip_8)}\")\n",
    "    #print(f\"url_zip_h: {type(url_zip_h)}\")\n",
    "    #print(f\"url_h: {type(url_h)}\")\n",
    "    #print(f\"url_txt: {type(url_txt)}\")\n",
    "    \n",
    "    sql = 'INSERT INTO LITERATURE (id, title, author, root_url, release_date, edition, language, produced_by, translator, url_txt_0, url_txt_8, url_zip_0, url_zip_8, url_zip_h, url_h, url_txt) values(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'\n",
    "    data = [(id, title, author, root_url, release_date, edition, language, produced_by, translator, url_txt_0, url_txt_8, url_zip_0, url_zip_8, url_zip_h, url_h, url_txt)]\n",
    "\n",
    "    with db_con:\n",
    "        db_con.executemany(sql, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con = sl.connect('gutenberg.db')\n",
    "\n",
    "with db_con:\n",
    "    db_con.execute(\"\"\"DROP table LITERATURE;\"\"\")\n",
    "\n",
    "    db_con.execute(\"\"\"\n",
    "        CREATE TABLE LITERATURE (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT,\n",
    "            author TEXT,\n",
    "            root_url TEXT,\n",
    "            release_date TEXT,\n",
    "            edition INTEGER,\n",
    "            language TEXT,\n",
    "            produced_by TEXT,\n",
    "            translator TEXT,\n",
    "            url_txt_0 TEXT,\n",
    "            url_txt_8 TEXT,\n",
    "            url_zip_0 TEXT,\n",
    "            url_zip_8 TEXT,\n",
    "            url_zip_h TEXT,\n",
    "            url_h TEXT,\n",
    "            url_txt TEXT\n",
    "        );\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Representations\n",
    "\n",
    "\n",
    "- One-Hot encoding\n",
    "    - Each word is represented as a single dimension\n",
    "\n",
    "![image.png](pic/fig_1_1.png)\n",
    "\n",
    "- Vector Space Model (VSM)\n",
    "    - Refered to as the \"Semantic Space\"\n",
    "    - Representation of the object is \"Distributed Representation\"\n",
    "        - object can be words, documents, sentences, concepts, or entities \n",
    "        \n",
    "![image.png](pic/fig_1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings\n",
    "\n",
    "\n",
    "Count-based models\n",
    "\n",
    "    - term-document (row: words, col: documents) denoting the frequency of a specific word in a doc. Meant for document centirc work (document retrieval, classification, or similar document)\n",
    "    - Word-context. (representing words) <---\n",
    "    - pair-pattern (row: pairs of words, col: pattern occurance)\n",
    "\n",
    "\n",
    "\n",
    "Positive Pointwise Mutual Infromation model\n",
    "\n",
    "    - better than raw frequencies.\n",
    "    - any negative values are clampped to 0\n",
    "    - checks if w_1 and w_2 co-occur more than they occur independelty\n",
    "    - negative values means that two words co-occurens is less than likely to happen than by chance\n",
    "\n",
    "$ PMI(w_1, w_2) = log_2 \\dfrac{P(w_1,w_2)}{P(w_1)P(w_2)}$\n",
    "\n",
    "\n",
    "\n",
    "Cosine Similarity\n",
    "\n",
    "    - dot product(v, w) where v and w are the embeddings of the word v and w.\n",
    "    - or the cos(v,w) = (v dot w)/|v||w|\n",
    "\n",
    "\n",
    "tf-idf model\n",
    "\n",
    "    - term frequency - the frequency of a word within a document\n",
    "    - inverse document frequency - the number of documents the words was found in\n",
    "    - collection frequency - the total number of times a words was found in all documents\n",
    "    - can look at the window as the \"document\"\n",
    "\n",
    "\n",
    "The simplest embeddings we can do is creating a matrix that is n by n, where n is the number of unique words in the corpus. Then use the PPMI metric to calculate the probability of the words occuring at all in the corpus.\n",
    "\n",
    "\n",
    "Another representation can be the word count of pairs of words, but only incrementing if the other word is +/- n words away from the target word. Essentially a sliding window, where the midpoint is the target word, and the outside words are the occurances of that word bing visable. This creates the impression of word relations between other words, with a small window allowing for specific connections, and larger windows give general connections.\n",
    "\n",
    "\n",
    "Dimensionality reduction can help with the previous matrix to reduce the sparsity of the matrix. (SVD and PCA). Note that the new columns are now not naturally interpretable, whereas the oriignal embedingnig can be interpretable.\n",
    "\n",
    "\n",
    "Recomended that we only keep track of the first 50,000 most frequent words, removing stop words, to reduce the sparsity of the embedding. Having more would just cause noise.\n",
    "\n",
    "\n",
    "Random Indexing\n",
    "\n",
    "    - \"RI first generates unique and random lowdimensional representations for words, called index vectors. Then, in an incremental process while scanning through the text corpus, each time a word c is observed in the context of a target word w, the index vector for w is updated by adding to it the index vector of c. \n",
    "    - The resulting representations are an approximation of the co-occurrence matrix, but with a significantly lower dimensionality, and without the need for costly SVD computations. Moreover, RI representations can be update after the initial training, and once new data is at disposal.\"\n",
    "\n",
    "\n",
    "Predictive Models\n",
    "\n",
    "    - Using neural networks to predict the embedding space of the words.\n",
    "    - Word2Vec\n",
    "    - GloVe\n",
    "\n",
    "Character Embeddings\n",
    "\n",
    "    - out of vocaulary (OOV)\n",
    "    - solution to randomly assign a value, but can hinder our understanding/accuracy\n",
    "    - using morphological segmenter (memoryless --> memory and less)\n",
    "    - broken into constituent subwords (groups of characters that are not neccessarily semantically meaningful)\n",
    "    - FastText\n",
    "    - Byte pair encoding, and WordPiece \n",
    "    - WordNet\n",
    "\n",
    "Knowledge-enhanced word embeddings\n",
    "    - retrofitting models\n",
    "\n",
    "\n",
    "Evaluation of word embeddings\n",
    "    - Intrinsic (generic evaluation)\n",
    "    - Extrinsic (specific application use evaluation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need an abstract class that holds the \"embeddings\" method\n",
    "# All we need to know is that the model works with an arbitrary embedding\n",
    "# which at the end of the day it is an n by n matrix.\n",
    "\n",
    "# This may be just a wrapper class for sklearn feature extraction \n",
    "\n",
    "# Sparse matrix representation by having 3 parallel lists\n",
    "# row holds the row index\n",
    "# col holds the col index\n",
    "# data holds the value at (row, col)\n",
    "class Embedding():\n",
    "    def __init__(self):\n",
    "        self.row = list()\n",
    "        self.col = list()\n",
    "        self.data = list()\n",
    "\n",
    "\n",
    "\n",
    "# we will be working with sparse matrixies, and thus we can use scipy's sparce matrix package\n",
    "# example of using their scipy.sparse.csr_array (Compressed Sparse Row array)\n",
    "# row = np.array([0, 0, 1, 2, 2, 2])\n",
    "# col = np.array([0, 2, 2, 0, 1, 2])\n",
    "# data = np.array([1, 2, 3, 4, 5, 6])\n",
    "# csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
    "#    array([[1, 0, 2],\n",
    "#           [0, 0, 3],\n",
    "#           [4, 5, 6]])\n",
    "# \n",
    "# The tuple to represent row and col are seperated in the row and col parallel vectors.\n",
    "# Then a third parallel vector represents the data that we are interested in\n",
    "# \n",
    "# Addtionlay, if there is a duplicate row/col entries, these are summed together\n",
    "# row = np.array([0, 1, 2, 0])\n",
    "# col = np.array([0, 1, 1, 0])\n",
    "# data = np.array([1, 2, 4, 8])\n",
    "# csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
    "#   array([[9, 0, 0],\n",
    "#          [0, 2, 0],\n",
    "#          [0, 4, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how to use sklearn feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "    unique_word_list = []\n",
    "    for x in list1:\n",
    "        sentence_list = x.split()\n",
    "        for y in sentence_list:\n",
    "            lower = y.lower()\n",
    "            if lower not in unique_word_list:\n",
    "                unique_word_list.append(lower)\n",
    "    return unique_word_list\n",
    "\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document',\n",
    "    'This is the second second document',\n",
    "    'And the third one',\n",
    "    'Is this the first document',\n",
    "]\n",
    "\n",
    "vocab_list = unique(corpus)\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=vocab_dict)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "\n",
    "Getting File paths to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import os \n",
    "import enchant\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial import distance\n",
    "from datetime import datetime\n",
    "\n",
    "# HELPPER FUNCTIONS\n",
    "\n",
    "# Given a directory path, get all of the file names and join the path to create an absolute path\n",
    "def GetListOfFileNames(dir_path):\n",
    "    return [os.path.join(dir_path, f) for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "\n",
    "\n",
    "# Calculate a the projection of vector a onto b, where both a and b are scipy sprase matrix\n",
    "def VectorProjection(a, b):\n",
    "    numerator = np.dot(a, b)\n",
    "    #print(numerator.shape)\n",
    "    denominator = np.dot(b.T, b)\n",
    "    #print(denominator.shape)\n",
    "    frac = numerator/denominator\n",
    "    return np.multiply(b, frac)\n",
    "\n",
    "\n",
    "def Distance(a, b):\n",
    "    #print(f\"a: {a.shape}\")\n",
    "    #print(f\"b: {b.shape}\")\n",
    "    diff = a - b\n",
    "    sum_square = np.dot(diff, diff.T)\n",
    "    #print(f\"sum_square: {sum_square.shape}\")\n",
    "    return np.sqrt(sum_square)\n",
    "\n",
    "\n",
    "# Global constant variables\n",
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "DIR_OF_TEXT = os.path.join(CURRENT_DIRECTORY, \"en_data/text\")\n",
    "FILE_PATH_TEXTS = GetListOfFileNames(DIR_OF_TEXT)\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "EN_DICT = enchant.Dict(\"en_US\")\n",
    "\n",
    "POLAR_WORD_SETS =[\n",
    "    [\"negative\", \"positive\"],\n",
    "    [\"displeased\", \"pleased\"],\n",
    "    [\"disapproving\", \"approving\"],\n",
    "    [\"disliking\", \"liking\"],\n",
    "    [\"fear\", \"hope\"],\n",
    "    [\"distress\", \"joy\"],\n",
    "    [\"shame\", \"pride\"],\n",
    "    [\"reproach\", \"admiration\"],\n",
    "    [\"hate\", \"love\"],\n",
    "    [\"disgust\", \"interest\"],\n",
    "    [\"fears-confirmed\", \"satisfaction\"],\n",
    "    [\"disappointment\", \"relief\"],\n",
    "    [\"resentment\", \"happy-for\"],\n",
    "    [\"pity\", \"gloating\"]\n",
    "]\n",
    "\n",
    "polar_word_index =[\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing data into embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init document-token matrix\n",
    "embedding_sets = {\n",
    "    \"count_vectorizer\": text.CountVectorizer,\n",
    "    \"tfidf_vectorizer\": text.TfidfVectorizer,\n",
    "    \"hashing_vectorizer\": text.HashingVectorizer\n",
    "}\n",
    "\n",
    "\n",
    "CURRENT_SET = \"count_vectorizer\"\n",
    "#WORDS = {}\n",
    "#idx = 0\n",
    "#for v, k in enumerate(words.words()):\n",
    "#    word = k.lower()\n",
    "#    if word not in WORDS.keys():\n",
    "#        WORDS[word] = idx\n",
    "#        idx += 1\n",
    "\n",
    "vectorizer_spgc = embedding_sets[CURRENT_SET](  input=\"filename\", \n",
    "                                    encoding='utf-8',\n",
    "                                    decode_error='ignore',\n",
    "                                    strip_accents='unicode',\n",
    "                                    lowercase=True,\n",
    "                                    stop_words=STOP_WORDS,\n",
    "                                    ngram_range = (1, 1),\n",
    "                                    analyzer='word',\n",
    "                                    dtype=np.float64)\n",
    "\n",
    "# Construct document-token matrix, then transpose for token-document matrix\n",
    "count_embeding = vectorizer_spgc.fit_transform(FILE_PATH_TEXTS)\n",
    "count_embeding = count_embeding.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"term_freq_document shape: {count_embeding.shape}\")\n",
    "feature_list = vectorizer_spgc.get_feature_names_out()\n",
    "\n",
    "sub_word_list = list()\n",
    "for token in feature_list:\n",
    "    if any(char.isdigit() for char in token):\n",
    "        continue\n",
    "    if EN_DICT.check(token):\n",
    "        sub_word_list.append(token)\n",
    "\n",
    "fraction_of_words = len(sub_word_list)/len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the 6077522 tokens, only 100024 are words. A 98.35419764831785% reduction\n"
     ]
    }
   ],
   "source": [
    "print(f\"From the {len(feature_list)} tokens, only {len(sub_word_list)} are words. A {(1.0 - fraction_of_words) * 100.0}% reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalculating embeding space with the subset of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {word.lower(): idx for idx, word in enumerate(sub_word_list)}\n",
    "\n",
    "vectorizer_spgc = embedding_sets[CURRENT_SET](  input=\"filename\", \n",
    "                                                encoding='utf-8',\n",
    "                                                decode_error='ignore',\n",
    "                                                strip_accents='unicode',\n",
    "                                                lowercase=True,\n",
    "                                                stop_words=STOP_WORDS,\n",
    "                                                ngram_range = (1, 1),\n",
    "                                                analyzer='word',\n",
    "                                                vocabulary= vocab_dict,\n",
    "                                                dtype=np.float64)\n",
    "\n",
    "# Construct document-token matrix, then transpose for token-document matrix\n",
    "count_embeding = vectorizer_spgc.fit_transform(FILE_PATH_TEXTS)\n",
    "count_embeding = count_embeding.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term_freq_document shape: (100024, 19369)\n",
      "negative at index 58054\n",
      "positive at index 67101\n",
      "displeased at index 25911\n",
      "pleased at index 66092\n",
      "disapproving at index 25212\n",
      "approving at index 4291\n",
      "disliking at index 25752\n",
      "liking at index 51006\n",
      "fear at index 32852\n",
      "hope at index 42358\n",
      "distress at index 26239\n",
      "joy at index 48296\n",
      "shame at index 79265\n",
      "pride at index 68285\n",
      "reproach at index 73935\n",
      "admiration at index 1198\n",
      "hate at index 40568\n",
      "love at index 51970\n",
      "disgust at index 25626\n",
      "interest at index 46596\n",
      "satisfaction at index 77039\n",
      "disappointment at index 25204\n",
      "relief at index 73320\n",
      "resentment at index 74150\n",
      "pity at index 65752\n",
      "gloating at index 37926\n",
      "[[58054, 67101], [25911, 66092], [25212, 4291], [25752, 51006], [32852, 42358], [26239, 48296], [79265, 68285], [73935, 1198], [40568, 51970], [25626, 46596], [-1, 77039], [25204, 73320], [74150, -1], [65752, 37926]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"term_freq_document shape: {count_embeding.shape}\")\n",
    "feature_list = vectorizer_spgc.get_feature_names_out()\n",
    "\n",
    "# From the vocabulary created by CountVectorizer, find the token index. If not found, keep -1\n",
    "for word_set_idx in range(len(POLAR_WORD_SETS)):\n",
    "    for word_idx in range(len(POLAR_WORD_SETS[word_set_idx])):\n",
    "        for (index, item) in enumerate(feature_list):\n",
    "            if item == POLAR_WORD_SETS[word_set_idx][word_idx]:\n",
    "                print(f\"{item} at index {index}\")\n",
    "                polar_word_index[word_set_idx][word_idx] = index\n",
    "                break\n",
    "print(polar_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count_embeding is a document-term matrix, where each row is a document and each featrue is a token. We may want to transpose this so that the first index is the word, and the second index is the document.\n",
    "\n",
    "From here, we want to perform vector arithmatic to determine how \"similar\" the words are in relation to a polar set of words. To do this we will need to:\n",
    "\n",
    "    - Get the polar word vectors (hate/love)\n",
    "    - Determine the vector between these by subtracting each other (love - hate)\n",
    "    - SemAxis will then need to be normalized ([love-hate]/|[love-hate]|)\n",
    "    - Project all terms vectors onto this SemAxis\n",
    "    - Determine the distance of the projected term from the negative word\n",
    "    - normalize distance such that it is from [0-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](pic/revised_occ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above list, we can see which words are in the embeddings space and which are not. Suspicion confrimed that the emotional words \"fears-confrimed\" and \"happy-for\" is not in the embedding. Will need to find a replacement word.\n",
    "\n",
    "Transpose the document-term matrix to term-document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the negative and positive tokens, subtrack their vectors to generate a SemAxis and normalize the vector. Below is the semaxis for the polar words (Negative, Positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project all token vectors onto the SemAxis vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time, we will use scikit learn's distance matrix function to calculate the distance between each word and midpoint polar set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_set_midpoints = list()\n",
    "for word_set_idx in range(len(POLAR_WORD_SETS)):\n",
    "    negative_idx = polar_word_index[word_set_idx][0] \n",
    "    positive_idx = polar_word_index[word_set_idx][1]\n",
    "    negative_str = POLAR_WORD_SETS[word_set_idx][0]\n",
    "    positive_str = POLAR_WORD_SETS[word_set_idx][1]\n",
    "    \n",
    "    if negative_idx == -1 or positive_idx == -1:\n",
    "        continue \n",
    "    \n",
    "    negative_point = count_embeding[negative_idx].toarray()\n",
    "    positive_point = count_embeding[positive_idx].toarray()\n",
    "\n",
    "    pos_neg_vector = positive_point - negative_point\n",
    "    semaxis_norm = pos_neg_vector / np.sqrt(np.sum(pos_neg_vector**2))\n",
    "    midpoint = pos_neg_vector/2.0\n",
    "\n",
    "    polar_set_midpoints.append(midpoint[0])\n",
    "\n",
    "polar_set_midpoints = np.asarray(polar_set_midpoints, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'c:\\\\Users\\\\Jake Klinkert\\\\Documents\\\\School\\\\Spring 2022\\\\CS8390 NLP\\\\final project\\\\code\\\\en_data/count_vectorizer/negative_positive.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JAKEKL~1\\AppData\\Local\\Temp/ipykernel_15620/2443759513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_for_polar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"token, distance\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'c:\\\\Users\\\\Jake Klinkert\\\\Documents\\\\School\\\\Spring 2022\\\\CS8390 NLP\\\\final project\\\\code\\\\en_data/count_vectorizer/negative_positive.csv'"
     ]
    }
   ],
   "source": [
    "dir_for_set = os.path.join(CURRENT_DIRECTORY, f\"en_data/{CURRENT_SET}/\")\n",
    "if not os.path.isdir(dir_for_set):\n",
    "    os.mkdir(dir_for_set)\n",
    "\n",
    "start=datetime.now()\n",
    "for word_set_idx in range(len(POLAR_WORD_SETS)):\n",
    "    negative_idx = polar_word_index[word_set_idx][0] \n",
    "    positive_idx = polar_word_index[word_set_idx][1]\n",
    "    negative_str = POLAR_WORD_SETS[word_set_idx][0]\n",
    "    positive_str = POLAR_WORD_SETS[word_set_idx][1]\n",
    "    dir_for_polar = os.path.join(dir_for_set, f\"{negative_str}_{positive_str}.csv\")\n",
    "    \n",
    "    if negative_idx == -1 or positive_idx == -1:\n",
    "        continue \n",
    "\n",
    "    with open(dir_for_polar, 'w') as f:\n",
    "        f.write(f\"token, distance\\n\")\n",
    "    \n",
    "\n",
    "        print(f\"Calculating SemAxis: {negative_str}_{positive_str}\")\n",
    "        \n",
    "        negative_point = count_embeding[negative_idx].toarray()\n",
    "        positive_point = count_embeding[positive_idx].toarray()\n",
    "        #print(negative_point.shape)\n",
    "        #print(positive_point.shape)\n",
    "\n",
    "        pos_neg_vector = positive_point - negative_point\n",
    "        semaxis_norm = pos_neg_vector / np.sqrt(np.sum(pos_neg_vector**2))\n",
    "        midpoint = pos_neg_vector/2.0\n",
    "\n",
    "        max_dist = 0.0\n",
    "        word_to_write = list()\n",
    "        num_words = 0\n",
    "        current_idx = 0\n",
    "        for idex in range(count_embeding.shape[0]):\n",
    "\n",
    "            if idex % 100000 == 0:\n",
    "                print(f\"Processed: {idex} tokens, found {num_words} words after {datetime.now()-start}\")\n",
    "                for words in word_to_write:\n",
    "                    f.write(f\"{words[0]},{words[1]}\\n\")\n",
    "                word_to_write.clear()\n",
    "\n",
    "            token_string = feature_list[idex]\n",
    "\n",
    "            if any(char.isdigit() for char in token_string):\n",
    "                continue\n",
    "\n",
    "            if not EN_DICT.check(token_string):\n",
    "                continue\n",
    "\n",
    "            projected_point = VectorProjection(count_embeding[idex].toarray(), semaxis_norm.T)\n",
    "            #print(f\"midpoint: {midpoint.shape}\")\n",
    "            #print(f\"projected_point: {projected_point.shape}\")\n",
    "            dot_product = np.dot(projected_point.T, midpoint.T)\n",
    "            #print(f\"dot_product: {dot_product.shape}\")\n",
    "            #print(f\"projected_point.T: {projected_point.T.shape}\")\n",
    "            #print(f\"midpoint: {midpoint.shape}\")\n",
    "            dist = Distance(projected_point.T, midpoint)\n",
    "\n",
    "            if max_dist < dist:\n",
    "                max_dist = dist\n",
    "\n",
    "            if dot_product < 0.0:\n",
    "                dist = -dist\n",
    "\n",
    "            word_to_write.append((token_string, dist))\n",
    "            num_words += 1\n",
    "            #print(f\"{token_string},{token_distances[token_string]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in token_distances:\n",
    "    token_distances[key] = token_distances[key]/max_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100024"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4740d0145867094702976d0dbc34e67e72e4d40ff6121e4f8a0bb27b6530e175"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

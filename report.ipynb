{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Valence Assignment for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_known_file = 1\n",
    "last_known_file = 67790\n",
    "#last_known_file = 1000\n",
    "max_file_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gutenberg.org/files/67770/67770-0.txt: 33048 / 33048\n",
      "https://gutenberg.org/files/67771/67771-0.txt: 33048 / 33048\n",
      "https://gutenberg.org/files/67772/67772-0.txt: 11288 / 33048\n",
      "https://gutenberg.org/files/67773/67773-0.txt: 95864 / 95864\n",
      "https://gutenberg.org/files/67774/67774-0.txt: 85176 / 95864\n",
      "https://gutenberg.org/files/67775/67775-0.txt: 37208 / 95864\n",
      "https://gutenberg.org/files/67776/67776-0.txt: 29336 / 95864\n",
      "https://gutenberg.org/files/67777/67777-0.txt: 29336 / 95864\n",
      "https://gutenberg.org/files/67778/67778-0.txt: 95864 / 95864\n",
      "https://gutenberg.org/files/67779/67779-0.txt: 53080 / 95864\n",
      "https://gutenberg.org/files/67780/67780-0.txt: 59736 / 95864\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for web_idx in range(first_known_file, last_known_file):\n",
    "    url = f\"https://gutenberg.org/files/{web_idx}/\"\n",
    "    try:\n",
    "        file = urllib.request.urlopen(url)\n",
    "    except urllib.error.HTTPError as e: # If the page is forbiden, then move on\n",
    "        continue\n",
    "    except urllib.error.URLError as e: # if we loose connection, try again\n",
    "        time.sleep(4)\n",
    "        web_idx -= 4\n",
    "\n",
    "    # Looking for every .txt.utf-8 file given the above url\n",
    "    online_file = list()\n",
    "    for line in file:\n",
    "        decoded_line = None\n",
    "        try:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            continue\n",
    "\n",
    "        online_file.append(decoded_line)\n",
    "\n",
    "\n",
    "\n",
    "    # For each .txt.utf-8 file we need to create a new url to get to the file\n",
    "    res = [string for string in online_file if '.txt\">' in string]\n",
    "    txt_urls = list()\n",
    "    txt_file_name = None\n",
    "    for line in res:\n",
    "        result = re.search('<a href=\"(.*).txt\">', line)\n",
    "        if result != None:\n",
    "            num_groups = len(result.groups())\n",
    "\n",
    "            if num_groups > 0:\n",
    "                txt_file_name = f\"{result.group(1)}.txt\"\n",
    "                txt_urls.append(f\"{url}{txt_file_name}\")\n",
    "\n",
    "    # for each .txt.utf-8 url file, read in the file and get all of the text\n",
    "    txt_file_content = list()\n",
    "    line_idx = 0\n",
    "    for url_idx in range(len(txt_urls)):\n",
    "        url = txt_urls[url_idx]\n",
    "\n",
    "        if url == None or url == '':\n",
    "            continue\n",
    "\n",
    "        file = urllib.request.urlopen(url)\n",
    "        line_by_line = list()\n",
    "        for line in file:\n",
    "            try:\n",
    "                decoded_line = line.decode(\"utf-8\")\n",
    "            except UnicodeDecodeError as e:\n",
    "                continue\n",
    "            line_by_line.append(decoded_line)  \n",
    "        txt_file_content.append(line_by_line)\n",
    "\n",
    "        size_of_file = sys.getsizeof(line_by_line)\n",
    "        if size_of_file > max_file_size:\n",
    "            max_file_size = size_of_file\n",
    "        print(f\"{url}: {size_of_file} / {max_file_size}\")\n",
    "    \n",
    "    # Write file to local disk\n",
    "    #txt_file_content_np = np.asarray(txt_file_content, str)\n",
    "    #txt_file_content_np = txt_file_content_np.flatten()\n",
    "    if len(txt_file_content) >= 1 and len(txt_file_content[0]) >= 1:\n",
    "        with open(f'raw/{txt_file_name}','w') as f:\n",
    "            if f.tell() < 1024*1024*1024:\n",
    "                #pickle.dump(txt_file_content_np, f)\n",
    "                f.write(json.dumps(txt_file_content))\n",
    "                #for line in txt_file_content:\n",
    "                    #f.write('%s\\n' % line)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:8 And God called the firmament Heaven. And the evening and the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with open('raw/10.pkl','rb') as f:\n",
    "#    x = pickle.load(f)\n",
    "#    print(x[0][125])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example document\n",
    "\n",
    "\n",
    "## ---------------\n",
    "\n",
    "\n",
    "The Project Gutenberg EBook of Madame de Ferneuse, by Daniel Lesueur\n",
    "\n",
    "This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you'll have to check the laws of the country where you are located before using this ebook.\n",
    "\n",
    "Title: Madame de Ferneuse\n",
    "\n",
    "Author: Daniel Lesueur\n",
    "\n",
    "Release Date: March 21, 2016 [EBook #51515]\n",
    "\n",
    "Language: French\n",
    "\n",
    "Character set encoding: UTF-8\n",
    "\n",
    "*** START OF THIS PROJECT GUTENBERG EBOOK MADAME DE FERNEUSE ***\n",
    "\n",
    "*** END OF THIS PROJECT GUTENBERG EBOOK MADAME DE FERNEUSE ***\n",
    "\n",
    "***** This file should be named 51515-0.txt or 51515-0.zip ***** This and all associated files of various formats will be found in: http://www.gutenberg.org/5/1/5/1/51515/\n",
    "\n",
    "\n",
    "## ------------------\n",
    "\n",
    "The Project Gutenberg EBook of The Meaning of Infancy, by John Fiske\n",
    "\n",
    "This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org\n",
    "\n",
    "Title: The Meaning of Infancy\n",
    "\n",
    "Author: John Fiske\n",
    "\n",
    "Release Date: May 15, 2004 [EBook #12359]\n",
    "\n",
    "Language: English\n",
    "\n",
    "Character set encoding: ASCII\n",
    "\n",
    "*** START OF THIS PROJECT GUTENBERG EBOOK THE MEANING OF INFANCY ***\n",
    "\n",
    "*** END OF THIS PROJECT GUTENBERG EBOOK THE MEANING OF INFANCY ***\n",
    "\n",
    "## -------------------\n",
    "\n",
    "The Project Gutenberg eBook of Traumerei, by Charles Beaumont\n",
    "\n",
    "This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.\n",
    "\n",
    "Title: Traumerei\n",
    "\n",
    "Author: Charles Beaumont\n",
    "\n",
    "Release Date: February 7, 2022 [eBook #67341]\n",
    "\n",
    "Language: English\n",
    "\n",
    "Produced by: Greg Weeks, Mary Meehan and the Online Distributed Proofreading Team at http://www.pgdp.net\n",
    "\n",
    "*** START OF THE PROJECT GUTENBERG EBOOK TRAUMEREI ***\n",
    "\n",
    "*** END OF THE PROJECT GUTENBERG EBOOK TRAUMEREI ***\n",
    "\n",
    "## -------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Design\n",
    "Right now, we are downloading all of the files, saving them as raw array format as a json file.\n",
    "\n",
    "Alternative approach as relational database\n",
    "We can generate the hypterlink by walking a list from 0 to last_known_file. We would need to check what files they have. Then, save only the heading information in each document, and remember where the starting index of the literature as well as the ending index.  \n",
    "\n",
    "Format tipes:\n",
    "<ul>\n",
    "    <li>.txt</li>\n",
    "    <li>-0.txt</li>\n",
    "    <li>-8.txt</li>\n",
    "    <li>-h/</li>\n",
    "    <li>-0.zip</li>\n",
    "    <li>-8.zip</li>\n",
    "    <li>-h.zip</li>\n",
    "</ul>\n",
    "\n",
    "from there, we can read the txt file, assuming it can be decoded using utf-8. The reoccuring headings can allow us to save data entries into a relational database.\n",
    "Document: (some of these fields may not be nan)\n",
    "<ul>\n",
    "    <li>Title:</li>\n",
    "    <li>Author:</li>\n",
    "    <li>URL:</li>\n",
    "    <li>Release Date:</li>\n",
    "    <li>Edition:</li>\n",
    "    <li>Language:</li>\n",
    "    <li>Produced by:</li>\n",
    "    <li>Start_Of_doc_indx:</li>\n",
    "    <li>End_of_doc_indx:</li>\n",
    "    <li>(or, if we have the memory, the entire document, but for now assuming we are reading from a URL or File Stream)</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python and SQLite3\n",
    "\n",
    "Medium post by Christopher Tao: https://towardsdatascience.com/do-you-know-python-has-a-built-in-database-d553989c87bd\n",
    "\n",
    "Tutorialspoint python SQLlite: https://towardsdatascience.com/do-you-know-python-has-a-built-in-database-d553989c87bd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite database  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example code to work with sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sl.connect('my-test.db')\n",
    "\n",
    "with con:\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE USER (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT,\n",
    "            age INTEGER\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "sql = 'INSERT INTO USER (id, name, age) values(?, ?, ?)'\n",
    "data = [\n",
    "    (1, 'Alice', 21),\n",
    "    (2, 'Bob', 22),\n",
    "    (3, 'Chris', 23)\n",
    "]\n",
    "\n",
    "with con:\n",
    "    con.executemany(sql, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Alice', 21)\n",
      "(2, 'Bob', 22)\n"
     ]
    }
   ],
   "source": [
    "with con:\n",
    "    data = con.execute(\"SELECT * FROM USER WHERE age <= 22\")\n",
    "    for row in data:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skill = pd.DataFrame({\n",
    "    'user_id': [1,1,2,2,3,3,3],\n",
    "    'skill': ['Network Security', 'Algorithm Development', 'Network Security', 'Java', 'Python', 'Data Science', 'Machine Learning']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_skill.to_sql('SKILL', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>21</td>\n",
       "      <td>Algorithm Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>21</td>\n",
       "      <td>Network Security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>22</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>22</td>\n",
       "      <td>Network Security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Chris</td>\n",
       "      <td>23</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Chris</td>\n",
       "      <td>23</td>\n",
       "      <td>Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>Chris</td>\n",
       "      <td>23</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   name  age                  skill\n",
       "0        1  Alice   21  Algorithm Development\n",
       "1        1  Alice   21       Network Security\n",
       "2        2    Bob   22                   Java\n",
       "3        2    Bob   22       Network Security\n",
       "4        3  Chris   23           Data Science\n",
       "5        3  Chris   23       Machine Learning\n",
       "6        3  Chris   23                 Python"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql('''\n",
    "    SELECT s.user_id, u.name, u.age, s.skill \n",
    "    FROM USER u LEFT JOIN SKILL s ON u.id = s.user_id\n",
    "''', con)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql('USER_SKILL', con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Refactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con = sl.connect('gutenberg.db')\n",
    "\n",
    "with db_con:\n",
    "    db_con.execute(\"\"\"\n",
    "        CREATE TABLE LITERATURE (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT,\n",
    "            author TEXT,\n",
    "            root_url TEXT,\n",
    "            release_date TEXT,\n",
    "            edition INTEGER,\n",
    "            language TEXT,\n",
    "            produced_by TEXT,\n",
    "            translator TEXT,\n",
    "            url_txt_0 TEXT,\n",
    "            url_txt_8 TEXT,\n",
    "            url_zip_0 TEXT,\n",
    "            url_zip_8 TEXT,\n",
    "            url_zip_h TEXT,\n",
    "            url_h TEXT,\n",
    "            url_txt TEXT\n",
    "        );\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sql = 'INSERT INTO USER (id, name, age) values(?, ?, ?)'\n",
    "#data = [\n",
    "#    (1, 'Alice', 21),\n",
    "#    (2, 'Bob', 22),\n",
    "#    (3, 'Chris', 23)\n",
    "#]\n",
    "#\n",
    "#with con:\n",
    "#    con.executemany(sql, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db_con = sl.connect('gutenberg.db')\n",
    "#\n",
    "#with con:\n",
    "#    con.execute(\"\"\"DROP table LITERATURE;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_0_FILE = '-0.txt'\n",
    "TXT_8_FILE = '-8.txt'\n",
    "ZIP_0_FILE = '-0.zip'\n",
    "ZIP_8_FILE = '-8.zip'\n",
    "ZIP_H_FILE = '-h.zip'\n",
    "H_FILE = '-h/'\n",
    "TXT_FILE = '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for web_idx in range(1, last_known_file):\n",
    "    # variables to populate in database\n",
    "    id = web_idx\n",
    "    title = \"None\"\n",
    "    author = \"None\"\n",
    "    root_url = f\"https://gutenberg.org/files/{web_idx}/\"\n",
    "    release_date = \"None\"\n",
    "    edition = 0\n",
    "    language = \"None\"\n",
    "    produced_by = \"None\"\n",
    "    translator = \"None\"\n",
    "    url_txt_0 = \"None\"\n",
    "    url_txt_8 = \"None\"\n",
    "    url_zip_0 = \"None\"\n",
    "    url_zip_8 = \"None\"\n",
    "    url_zip_h = \"None\"\n",
    "    url_h = \"None\"\n",
    "    url_txt = \"None\"\n",
    "\n",
    "    # Connect to project gutenberg\n",
    "    try:\n",
    "        file = urllib.request.urlopen(root_url)\n",
    "    except urllib.error.HTTPError as e: # If the page is forbiden, then move on\n",
    "        continue\n",
    "    except urllib.error.URLError as e: # if we loose connection, try again\n",
    "        time.sleep(4)\n",
    "        web_idx -= 4\n",
    "\n",
    "    # Strip web string from php syntax\n",
    "    online_file = list()\n",
    "    for line in file:\n",
    "        decoded_line = None\n",
    "        try:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            continue\n",
    "        readable_line = ''.join(BeautifulSoup(decoded_line).findAll(text=True))\n",
    "        online_file.append(readable_line)\n",
    "    fullstring = ''.join(online_file)\n",
    "\n",
    "    # Parese web file for txt files  \n",
    "    if f'{web_idx}{TXT_0_FILE}' in fullstring:\n",
    "        url_txt_0 = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{TXT_0_FILE}\"\n",
    "    if f'{web_idx}{TXT_8_FILE}' in fullstring:\n",
    "        url_txt_8 = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{TXT_8_FILE}\"\n",
    "    if f'{web_idx}{ZIP_0_FILE}' in fullstring:\n",
    "        url_zip_0 = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{ZIP_0_FILE}\"\n",
    "    if f'{web_idx}{ZIP_8_FILE}' in fullstring:\n",
    "        url_zip_8 = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{ZIP_8_FILE}\"\n",
    "    if f'{web_idx}{ZIP_H_FILE}' in fullstring:\n",
    "        url_zip_h = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{ZIP_H_FILE}\"\n",
    "    if f'{web_idx}{H_FILE}' in fullstring:\n",
    "        url_h = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{H_FILE}\"\n",
    "    if f'{web_idx}{TXT_FILE}' in fullstring:\n",
    "        url_txt = f\"https://gutenberg.org/files/{web_idx}/{web_idx}{TXT_FILE}\"\n",
    "\n",
    "    # use one of the txt files to gather remanining variables\n",
    "    file_url = None\n",
    "    if url_txt_0:\n",
    "        file_url = url_txt_0\n",
    "    elif url_txt_8:\n",
    "        file_url = url_txt_8\n",
    "    elif url_txt:\n",
    "        file_url = url_txt\n",
    "\n",
    "    if file_url == \"None\":\n",
    "        continue\n",
    "\n",
    "    # Connect to txt file\n",
    "    file = None\n",
    "    try:\n",
    "        file = urllib.request.urlopen(file_url)\n",
    "    except urllib.error.HTTPError as e: # If the page is forbiden, then move on\n",
    "        continue\n",
    "    except urllib.error.URLError as e: # if we loose connection, try again\n",
    "        time.sleep(4)\n",
    "        web_idx -= 4\n",
    "\n",
    "    # Strip txt file from php syntax\n",
    "    txt_file = list()\n",
    "    for line in file:\n",
    "        decoded_line = None\n",
    "        try:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            continue\n",
    "        #readable_line = ''.join(BeautifulSoup(decoded_line).findAll(text=True))\n",
    "        txt_file.append(decoded_line)\n",
    "    full_txt = ''.join(txt_file)\n",
    "\n",
    "    #print(full_txt)\n",
    "\n",
    "    # Parse text file for\n",
    "    title_results = re.search('Title: (.*)', full_txt)\n",
    "    author_results = re.search('Author: (.*)', full_txt)\n",
    "    release_data_results = re.search('Release Date: (.*) \\[', full_txt)\n",
    "    edition_results = re.search('Edition: (.*)', full_txt)\n",
    "    language_results = re.search('Language: (.*)', full_txt)\n",
    "    produced_results = re.search('Produced by: (.*)', full_txt)\n",
    "    translator_results = re.search('Translator: (.*)', full_txt)\n",
    "\n",
    "    if title_results:\n",
    "        title = str(title_results.group(1)).strip()\n",
    "    if author_results:\n",
    "        author = str(author_results.group(1)).strip()\n",
    "    if release_data_results:\n",
    "        release_date = str(release_data_results.group(1)).strip()\n",
    "    if edition_results:\n",
    "        edition_str = str(edition_results.group(1)).strip()\n",
    "        if edition_str.isnumeric():\n",
    "            edition = int(edition_str)\n",
    "    if language_results:\n",
    "        language = str(language_results.group(1)).strip()\n",
    "    if produced_results:\n",
    "        produced_by = str(produced_results.group(1)).strip()\n",
    "    if translator_results:\n",
    "        translator = str(translator_results.group(1)).strip()\n",
    "\n",
    "\n",
    "    #print(f\"id: {type(id)}\")\n",
    "    #print(f\"title: {type(title)}\")\n",
    "    #print(f\"author: {type(author)}\")\n",
    "    #print(f\"root_url: {type(root_url)}\")\n",
    "    #print(f\"release_date: {type(release_date)}\")\n",
    "    #print(f\"edition: {type(edition)}\")\n",
    "    #print(f\"language: {type(language)}\")\n",
    "    #print(f\"produced_by: {type(produced_by)}\")\n",
    "    #print(f\"url_txt_0: {type(url_txt_0)}\")\n",
    "    #print(f\"url_txt_8: {type(url_txt_8)}\")\n",
    "    #print(f\"url_zip_0: {type(url_zip_0)}\")\n",
    "    #print(f\"url_zip_8: {type(url_zip_8)}\")\n",
    "    #print(f\"url_zip_h: {type(url_zip_h)}\")\n",
    "    #print(f\"url_h: {type(url_h)}\")\n",
    "    #print(f\"url_txt: {type(url_txt)}\")\n",
    "    \n",
    "    sql = 'INSERT INTO LITERATURE (id, title, author, root_url, release_date, edition, language, produced_by, translator, url_txt_0, url_txt_8, url_zip_0, url_zip_8, url_zip_h, url_h, url_txt) values(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'\n",
    "    data = [(id, title, author, root_url, release_date, edition, language, produced_by, translator, url_txt_0, url_txt_8, url_zip_0, url_zip_8, url_zip_h, url_h, url_txt)]\n",
    "\n",
    "    with db_con:\n",
    "        db_con.executemany(sql, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con = sl.connect('gutenberg.db')\n",
    "\n",
    "with db_con:\n",
    "    db_con.execute(\"\"\"DROP table LITERATURE;\"\"\")\n",
    "\n",
    "    db_con.execute(\"\"\"\n",
    "        CREATE TABLE LITERATURE (\n",
    "            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT,\n",
    "            author TEXT,\n",
    "            root_url TEXT,\n",
    "            release_date TEXT,\n",
    "            edition INTEGER,\n",
    "            language TEXT,\n",
    "            produced_by TEXT,\n",
    "            translator TEXT,\n",
    "            url_txt_0 TEXT,\n",
    "            url_txt_8 TEXT,\n",
    "            url_zip_0 TEXT,\n",
    "            url_zip_8 TEXT,\n",
    "            url_zip_h TEXT,\n",
    "            url_h TEXT,\n",
    "            url_txt TEXT\n",
    "        );\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Representations\n",
    "\n",
    "\n",
    "- One-Hot encoding\n",
    "    - Each word is represented as a single dimension\n",
    "\n",
    "![image.png](pic/fig_1_1.png)\n",
    "\n",
    "- Vector Space Model (VSM)\n",
    "    - Refered to as the \"Semantic Space\"\n",
    "    - Representation of the object is \"Distributed Representation\"\n",
    "        - object can be words, documents, sentences, concepts, or entities \n",
    "        \n",
    "![image.png](pic/fig_1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings\n",
    "\n",
    "\n",
    "Count-based models\n",
    "\n",
    "    - term-document (row: words, col: documents) denoting the frequency of a specific word in a doc. Meant for document centirc work (document retrieval, classification, or similar document)\n",
    "    - Word-context. (representing words) <---\n",
    "    - pair-pattern (row: pairs of words, col: pattern occurance)\n",
    "\n",
    "\n",
    "\n",
    "Positive Pointwise Mutual Infromation model\n",
    "\n",
    "    - better than raw frequencies.\n",
    "    - any negative values are clampped to 0\n",
    "    - checks if w_1 and w_2 co-occur more than they occur independelty\n",
    "    - negative values means that two words co-occurens is less than likely to happen than by chance\n",
    "\n",
    "$ PMI(w_1, w_2) = log_2 \\dfrac{P(w_1,w_2)}{P(w_1)P(w_2)}$\n",
    "\n",
    "\n",
    "\n",
    "Cosine Similarity\n",
    "\n",
    "    - dot product(v, w) where v and w are the embeddings of the word v and w.\n",
    "    - or the cos(v,w) = (v dot w)/|v||w|\n",
    "\n",
    "\n",
    "tf-idf model\n",
    "\n",
    "    - term frequency - the frequency of a word within a document\n",
    "    - inverse document frequency - the number of documents the words was found in\n",
    "    - collection frequency - the total number of times a words was found in all documents\n",
    "    - can look at the window as the \"document\"\n",
    "\n",
    "\n",
    "The simplest embeddings we can do is creating a matrix that is n by n, where n is the number of unique words in the corpus. Then use the PPMI metric to calculate the probability of the words occuring at all in the corpus.\n",
    "\n",
    "\n",
    "Another representation can be the word count of pairs of words, but only incrementing if the other word is +/- n words away from the target word. Essentially a sliding window, where the midpoint is the target word, and the outside words are the occurances of that word bing visable. This creates the impression of word relations between other words, with a small window allowing for specific connections, and larger windows give general connections.\n",
    "\n",
    "\n",
    "Dimensionality reduction can help with the previous matrix to reduce the sparsity of the matrix. (SVD and PCA). Note that the new columns are now not naturally interpretable, whereas the oriignal embedingnig can be interpretable.\n",
    "\n",
    "\n",
    "Recomended that we only keep track of the first 50,000 most frequent words, removing stop words, to reduce the sparsity of the embedding. Having more would just cause noise.\n",
    "\n",
    "\n",
    "Random Indexing\n",
    "\n",
    "    - \"RI first generates unique and random lowdimensional representations for words, called index vectors. Then, in an incremental process while scanning through the text corpus, each time a word c is observed in the context of a target word w, the index vector for w is updated by adding to it the index vector of c. \n",
    "    - The resulting representations are an approximation of the co-occurrence matrix, but with a significantly lower dimensionality, and without the need for costly SVD computations. Moreover, RI representations can be update after the initial training, and once new data is at disposal.\"\n",
    "\n",
    "\n",
    "Predictive Models\n",
    "\n",
    "    - Using neural networks to predict the embedding space of the words.\n",
    "    - Word2Vec\n",
    "    - GloVe\n",
    "\n",
    "Character Embeddings\n",
    "\n",
    "    - out of vocaulary (OOV)\n",
    "    - solution to randomly assign a value, but can hinder our understanding/accuracy\n",
    "    - using morphological segmenter (memoryless --> memory and less)\n",
    "    - broken into constituent subwords (groups of characters that are not neccessarily semantically meaningful)\n",
    "    - FastText\n",
    "    - Byte pair encoding, and WordPiece \n",
    "    - WordNet\n",
    "\n",
    "Knowledge-enhanced word embeddings\n",
    "    - retrofitting models\n",
    "\n",
    "\n",
    "Evaluation of word embeddings\n",
    "    - Intrinsic (generic evaluation)\n",
    "    - Extrinsic (specific application use evaluation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need an abstract class that holds the \"embeddings\" method\n",
    "# All we need to know is that the model works with an arbitrary embedding\n",
    "# which at the end of the day it is an n by n matrix.\n",
    "\n",
    "# This may be just a wrapper class for sklearn feature extraction \n",
    "\n",
    "# Sparse matrix representation by having 3 parallel lists\n",
    "# row holds the row index\n",
    "# col holds the col index\n",
    "# data holds the value at (row, col)\n",
    "class Embedding():\n",
    "    def __init__(self):\n",
    "        self.row = list()\n",
    "        self.col = list()\n",
    "        self.data = list()\n",
    "\n",
    "\n",
    "\n",
    "# we will be working with sparse matrixies, and thus we can use scipy's sparce matrix package\n",
    "# example of using their scipy.sparse.csr_array (Compressed Sparse Row array)\n",
    "# row = np.array([0, 0, 1, 2, 2, 2])\n",
    "# col = np.array([0, 2, 2, 0, 1, 2])\n",
    "# data = np.array([1, 2, 3, 4, 5, 6])\n",
    "# csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
    "#    array([[1, 0, 2],\n",
    "#           [0, 0, 3],\n",
    "#           [4, 5, 6]])\n",
    "# \n",
    "# The tuple to represent row and col are seperated in the row and col parallel vectors.\n",
    "# Then a third parallel vector represents the data that we are interested in\n",
    "# \n",
    "# Addtionlay, if there is a duplicate row/col entries, these are summed together\n",
    "# row = np.array([0, 1, 2, 0])\n",
    "# col = np.array([0, 1, 1, 0])\n",
    "# data = np.array([1, 2, 4, 8])\n",
    "# csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
    "#   array([[9, 0, 0],\n",
    "#          [0, 2, 0],\n",
    "#          [0, 4, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how to use sklearn feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0,\n",
       " 'is': 1,\n",
       " 'the': 2,\n",
       " 'first': 3,\n",
       " 'document': 4,\n",
       " 'second': 5,\n",
       " 'and': 6,\n",
       " 'third': 7,\n",
       " 'one': 8}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unique(list1):\n",
    "    unique_word_list = []\n",
    "    for x in list1:\n",
    "        sentence_list = x.split()\n",
    "        for y in sentence_list:\n",
    "            lower = y.lower()\n",
    "            if lower not in unique_word_list:\n",
    "                unique_word_list.append(lower)\n",
    "    return unique_word_list\n",
    "\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document',\n",
    "    'This is the second second document',\n",
    "    'And the third one',\n",
    "    'Is this the first document',\n",
    "]\n",
    "\n",
    "vocab_list = unique(corpus)\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 1, 2, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=vocab_dict)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def GetListOfFileNames(dir_path):\n",
    "    return [join(dir_path, f) for f in listdir(dir_path) if isfile(join(dir_path, f))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting File paths to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Jake Klinkert/Documents/School/Spring 2022/CS8390 NLP/final project/code/data/tokens\\\\.dummy'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_OF_TEXT = \"C:/Users/Jake Klinkert/Documents/School/Spring 2022/CS8390 NLP/final project/code/data/text\"\n",
    "DIR_OF_COUNTS = \"C:/Users/Jake Klinkert/Documents/School/Spring 2022/CS8390 NLP/final project/code/data/counts\"\n",
    "DIR_OF_TOKENS = \"C:/Users/Jake Klinkert/Documents/School/Spring 2022/CS8390 NLP/final project/code/data/tokens\"\n",
    "\n",
    "\n",
    "file_path_texts = GetListOfFileNames(DIR_OF_TEXT)\n",
    "file_path_texts.pop(0)\n",
    "\n",
    "file_path_counts = GetListOfFileNames(DIR_OF_COUNTS)\n",
    "file_path_counts.pop(0)\n",
    "\n",
    "file_path_tokens = GetListOfFileNames(DIR_OF_TOKENS)\n",
    "file_path_tokens.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing data into embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_spgc = CountVectorizer(  input=\"filename\", \n",
    "                                    encoding='utf-8',\n",
    "                                    strip_accents='ascii',\n",
    "                                    lowercase=True,\n",
    "                                    stop_words=stopwords.words('english'),\n",
    "                                    ngram_range = (1, 1))\n",
    "\n",
    "count_embeding = vectorizer_spgc.fit_transform(file_path_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count_embeding is a document-term matrix, where each row is a document and each featrue is a token. We may want to transpose this so that the first index is the word, and the second index is the document.\n",
    "\n",
    "From here, we want to perform vector arithmatic to determine how \"similar\" the words are in relation to a polar set of words. To do this we will need to:\n",
    "\n",
    "    - Get the polar word vectors (hate/love)\n",
    "    - Determine the vector between these by subtracting each other (love - hate)\n",
    "    - SemAxis will then need to be normalized ([love-hate]/|[love-hate]|)\n",
    "    - Project all terms vectors onto this SemAxis\n",
    "    - Determine the distance of the projected term from the negative word\n",
    "    - normalize distance such that it is from [0-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](pic/revised_occ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_word_sets =[\n",
    "    [\"negative\", \"positive\"],\n",
    "    [\"displeased\", \"pleased\"],\n",
    "    [\"disapproving\", \"approving\"],\n",
    "    [\"disliking\", \"liking\"],\n",
    "    [\"fear\", \"hope\"],\n",
    "    [\"distress\", \"joy\"],\n",
    "    [\"shame\", \"pride\"],\n",
    "    [\"reproach\", \"admiration\"],\n",
    "    [\"hate\", \"love\"],\n",
    "    [\"disgust\", \"interest\"],\n",
    "    [\"fears-confirmed\", \"satisfaction\"],\n",
    "    [\"disappointment\", \"relief\"],\n",
    "    [\"resentment\", \"happy-for\"],\n",
    "    [\"pity\", \"gloating\"]\n",
    "]\n",
    "\n",
    "polar_word_index =[\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1],\n",
    "    [-1, -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = vectorizer_spgc.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative at index 7369506\n",
      "positive at index 8030569\n",
      "displeased at index 3928681\n",
      "pleased at index 7966067\n",
      "disapproving at index 3912933\n",
      "approving at index 1964729\n",
      "disliking at index 3924900\n",
      "liking at index 6666615\n",
      "fear at index 4440214\n",
      "hope at index 5743119\n",
      "distress at index 3937670\n",
      "joy at index 6103383\n",
      "shame at index 8811288\n",
      "pride at index 8091041\n",
      "reproach at index 8387214\n",
      "admiration at index 1530527\n",
      "hate at index 5565653\n",
      "love at index 6735188\n",
      "disgust at index 3922027\n",
      "interest at index 5963450\n",
      "satisfaction at index 8633246\n",
      "disappointment at index 3912733\n",
      "relief at index 8360014\n",
      "resentment at index 8393765\n",
      "pity at index 7950937\n",
      "gloating at index 5220677\n",
      "[[7369506, 8030569], [3928681, 7966067], [3912933, 1964729], [3924900, 6666615], [4440214, 5743119], [3937670, 6103383], [8811288, 8091041], [8387214, 1530527], [5565653, 6735188], [3922027, 5963450], [-1, 8633246], [3912733, 8360014], [8393765, -1], [7950937, 5220677]]\n"
     ]
    }
   ],
   "source": [
    "for word_set_idx in range(len(polar_word_sets)):\n",
    "    for word_idx in range(len(polar_word_sets[word_set_idx])):\n",
    "        for (index, item) in enumerate(feature_list):\n",
    "            if item == polar_word_sets[word_set_idx][word_idx]:\n",
    "                print(f\"{item} at index {index}\")\n",
    "                polar_word_index[word_set_idx][word_idx] = index\n",
    "                break\n",
    "\n",
    "print(polar_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above list, we can see which words are in the embeddings space and which are not. Suspicion confrimed that the emotional words \"fears-confrimed\" and \"happy-for\" is not in the embedding. Will need to find a replacement word.\n",
    "\n",
    "Transpose the document-term matrix to term-document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11064651x24530 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 172907264 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_count_embeding = count_embeding.transpose()\n",
    "t_count_embeding = t_count_embeding.astype(np.float64)\n",
    "t_count_embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the negative and positive tokens, subtrack their vectors to generate a SemAxis and normalize the vector. Below is the semaxis for the polar words (Negative, Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.0021937989419186853\n",
      "  (0, 5)\t0.0010968994709593426\n",
      "  (0, 6)\t0.0043875978838373705\n",
      "  (0, 7)\t0.0032906984128780277\n",
      "  (0, 9)\t0.0010968994709593426\n",
      "  (0, 13)\t0.0021937989419186853\n",
      "  (0, 14)\t0.0032906984128780277\n",
      "  (0, 16)\t0.0010968994709593426\n",
      "  (0, 18)\t-0.0010968994709593426\n",
      "  (0, 19)\t0.0032906984128780277\n",
      "  (0, 20)\t0.0010968994709593426\n",
      "  (0, 24)\t0.007678296296715398\n",
      "  (0, 26)\t0.0010968994709593426\n",
      "  (0, 27)\t0.0021937989419186853\n",
      "  (0, 28)\t0.0010968994709593426\n",
      "  (0, 29)\t-0.0021937989419186853\n",
      "  (0, 35)\t0.0032906984128780277\n",
      "  (0, 36)\t0.0021937989419186853\n",
      "  (0, 37)\t0.0021937989419186853\n",
      "  (0, 40)\t0.0021937989419186853\n",
      "  (0, 41)\t-0.0032906984128780277\n",
      "  (0, 46)\t0.0010968994709593426\n",
      "  (0, 49)\t0.0032906984128780277\n",
      "  (0, 53)\t0.0032906984128780277\n",
      "  (0, 58)\t0.005484497354796713\n",
      "  :\t:\n",
      "  (0, 24463)\t0.0032906984128780277\n",
      "  (0, 24465)\t-0.0010968994709593426\n",
      "  (0, 24473)\t0.005484497354796713\n",
      "  (0, 24475)\t0.008775195767674741\n",
      "  (0, 24479)\t-0.0010968994709593426\n",
      "  (0, 24480)\t-0.0010968994709593426\n",
      "  (0, 24484)\t0.0010968994709593426\n",
      "  (0, 24485)\t0.0032906984128780277\n",
      "  (0, 24486)\t0.0010968994709593426\n",
      "  (0, 24487)\t0.010968994709593427\n",
      "  (0, 24488)\t-0.006581396825756055\n",
      "  (0, 24489)\t-0.006581396825756055\n",
      "  (0, 24492)\t0.006581396825756055\n",
      "  (0, 24494)\t0.0010968994709593426\n",
      "  (0, 24498)\t0.0032906984128780277\n",
      "  (0, 24500)\t-0.0032906984128780277\n",
      "  (0, 24503)\t0.0021937989419186853\n",
      "  (0, 24504)\t0.0021937989419186853\n",
      "  (0, 24507)\t0.0010968994709593426\n",
      "  (0, 24510)\t0.0010968994709593426\n",
      "  (0, 24515)\t0.0032906984128780277\n",
      "  (0, 24517)\t0.0021937989419186853\n",
      "  (0, 24522)\t0.0010968994709593426\n",
      "  (0, 24523)\t0.0043875978838373705\n",
      "  (0, 24525)\t0.0032906984128780277\n"
     ]
    }
   ],
   "source": [
    "negative_vector = t_count_embeding[polar_word_index[0][0]]\n",
    "positive_vector = t_count_embeding[polar_word_index[0][1]]\n",
    "semaxis = positive_vector - negative_vector\n",
    "semaxis_norm = normalize(semaxis, norm='l2', axis=1)\n",
    "print(semaxis_norm[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project all token vectors onto the SemAxis vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorProjection(a, b):\n",
    "    numerator = a.dot(b)\n",
    "    denominator = b.transpose().dot(b)\n",
    "    frac = numerator/denominator\n",
    "    return b.multiply(frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "projected_points = t_count_embeding.copy()\n",
    "print(projected_points.dtype)\n",
    "\n",
    "for idex in range(t_count_embeding.shape[0]):\n",
    "    projection = VectorProjection(t_count_embeding[idex], semaxis_norm.transpose())\n",
    "    projected_points[idex] = projection.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07578402767332579\n"
     ]
    }
   ],
   "source": [
    "test_vector = projected_points[0].toarray()\n",
    "print(test_vector[0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown Distance Metric: euclidian",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JAKEKL~1\\AppData\\Local\\Temp/ipykernel_34792/3439361678.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m scipy.spatial.distance.cdist(   XA= test_vector,\n\u001b[0m\u001b[0;32m      4\u001b[0m                                 \u001b[0mXB\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msemaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 metric=\"euclidian\")\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mcdist\u001b[1;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[0;32m   2962\u001b[0m                 XA, XB, metric=metric_info.dist_func, out=out, **kwargs)\n\u001b[0;32m   2963\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2964\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown Distance Metric: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2966\u001b[0m         raise TypeError('2nd argument metric must be a string identifier '\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown Distance Metric: euclidian"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "scipy.spatial.distance.cdist(   XA= test_vector,\n",
    "                                XB= semaxis.toarray(),\n",
    "                                metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4740d0145867094702976d0dbc34e67e72e4d40ff6121e4f8a0bb27b6530e175"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
